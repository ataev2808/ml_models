{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qfmsbqvAUR2EEhq5xZrmWxPn3bGRu0I3",
      "authorship_tag": "ABX9TyPfp+CpR37snbE2BF2ZlHJ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ataev2808/nlp_models/blob/main/review_classification_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrbYgaSfBTpe"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "import sklearn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv')"
      ],
      "metadata": {
        "id": "pcupwihOeQBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#инфа о датасете\n",
        "df.shape\n",
        "df.info()\n",
        "df.duplicated().sum()\n",
        "df.isnull().sum()\n",
        "df[df.duplicated(keep=False)].sort_values('review')"
      ],
      "metadata": {
        "id": "hirsAB2tehtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud"
      ],
      "metadata": {
        "id": "JDutoPuLangW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# облако слов до очистки\n",
        "\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "merged_text = ' '.join(df['review'])\n",
        "wordcloud = WordCloud(width=800, height=400).generate(merged_text)\n",
        "plt.imshow(wordcloud)\n"
      ],
      "metadata": {
        "id": "pUH6g855aUwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# удаляю пустые значения, дубликаты, сбрасываю индексы\n",
        "\n",
        "df = df.dropna(subset=['review', 'sentiment'])\n",
        "df = df[df['review'].str.strip() != '']\n",
        "df = df.drop_duplicates()\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "df.shape"
      ],
      "metadata": {
        "id": "rdqFwDZYi7MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# очищаю текст\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    text = re.sub(r\"[`´‘’“”„‟„`''``]+\", '', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s!?()]', '', text)   # оставил скобки, типо смайлы\n",
        "    return text.lower()\n",
        "\n",
        "clean_text = df['review'].apply(clean_text)\n",
        "print(len(clean_text))\n"
      ],
      "metadata": {
        "id": "ldLRiEM7BpIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# анализ баланса классов\n",
        "plt.figure(figsize=(4, 4))\n",
        "df['sentiment'].value_counts().plot.pie(autopct='%1.1f%%', colors=['coral','beige'])\n",
        "plt.title('Процентное распределение классов')"
      ],
      "metadata": {
        "id": "F_ihnfkIj26m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# токенизирую, удаляю стоп-слова\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "tokenize_text = [nltk.word_tokenize(sentence) for sentence in clean_text]\n",
        "tokenize_text_wsws = [[word for word in sentence if word not in stop_words] for sentence in tokenize_text]\n",
        "\n",
        "print(tokenize_text_wsws)\n",
        "print(len(tokenize_text))"
      ],
      "metadata": {
        "id": "bVjpV7MGnjYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# лемматизация\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_text = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in tokenize_text_wsws]\n",
        "# print(lemmatized_text)\n",
        "\n",
        "# обединяем токены в предложения\n",
        "final_text = [\" \".join(sentence) for sentence in lemmatized_text]\n",
        "# print(final_text)"
      ],
      "metadata": {
        "id": "Sl1b5dYtGVQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# облако слов после очистки\n",
        "\n",
        "merged_text = ' '.join(final_text)\n",
        "wordcloud = WordCloud(width=800, height=400).generate(merged_text)\n",
        "plt.imshow(wordcloud)"
      ],
      "metadata": {
        "id": "qhRVBTrkcYTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# векторизация текста c помощью bag of words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vec = CountVectorizer(ngram_range=(1, 2))\n",
        "bag_of_words = count_vec.fit_transform(sentence for sentence in final_text) #мб надо создать глобальный словарь по всему корпусу\n",
        "bag_of_words\n",
        "\n"
      ],
      "metadata": {
        "id": "lcc5P7vzClU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# векторизация текста c помощью tf-idf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(ngram_range=(1, 2))\n",
        "tfidf = tfidf_vec.fit_transform(sentence for sentence in final_text)\n",
        "tfidf\n",
        "# tfidf_df =  pd.DataFrame(tfidf.toarray(), columns=tfidf_vec.get_feature_names_out(), index=final_text)\n",
        "# tfidf_df"
      ],
      "metadata": {
        "id": "1F7LirWND2De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение модели логистической регрессии\n",
        "# разделение датасета на обучающий и тестовый наборы\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df['final_text'] = [\" \".join(sentence) for sentence in lemmatized_text]\n",
        "# print(\"Размер df:\", df.shape)\n",
        "# print(\"Количество текстов:\", len(df['final_text']))\n",
        "X = df['final_text']\n",
        "y = df['sentiment']\n",
        "y = y.map({'positive': 1, 'negative': 0})\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "# print(X)\n",
        "# print(y)"
      ],
      "metadata": {
        "id": "uFvfGTZBYaUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# выбор n-грамм\n",
        "n1 = (1, 1)\n",
        "n2 = (1, 2)"
      ],
      "metadata": {
        "id": "nauqce1sVFJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# векторизация обучающего набора методом bow\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "vec = CountVectorizer(ngram_range=n2)\n",
        "bow_train = vec.fit_transform(X_train)\n",
        "bow_test = vec.transform(X_test)\n",
        "\n",
        "# нормализация\n",
        "scaler = MaxAbsScaler()\n",
        "bow_train = scaler.fit_transform(bow_train)\n",
        "bow_test = scaler.transform(bow_test)\n",
        "\n",
        "# обучение с bow и лемматизацией\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "clf = LogisticRegression(max_iter=200, random_state=42)\n",
        "clf.fit(bow_train, y_train)\n",
        "pred = clf.predict(bow_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "metadata": {
        "id": "swXyJnOxXld4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "\n",
        "classes = ['Negative', 'Positive']\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Матрица ошибок')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes, rotation=45)\n",
        "plt.yticks(tick_marks, classes)\n",
        "thresh = cm.max() / 2.\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.ylabel('Настоящий класс')\n",
        "plt.xlabel('Предсказанный класс')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dazS_ET1sPGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# векторизация обучающего набора методом tf-idf\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "vec = TfidfVectorizer(ngram_range=n2)\n",
        "vec_train = vec.fit_transform(X_train)\n",
        "vec_test = vec.transform(X_test)\n",
        "\n",
        "# нормализация\n",
        "scaler = MaxAbsScaler()\n",
        "vec_train = scaler.fit_transform(vec_train)\n",
        "vec_test = scaler.transform(vec_test)\n",
        "\n",
        "# обучение с tf-idf и лемматизацией\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "clf = LogisticRegression(max_iter=200, random_state=42)\n",
        "clf.fit(vec_train, y_train)\n",
        "pred = clf.predict(vec_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "# сохраняю веса\n",
        "import joblib\n",
        "\n",
        "joblib.dump(vec,       'tfidf_vectorizer.pkl')\n",
        "joblib.dump(scaler,    'maxabs_scaler.pkl')\n",
        "joblib.dump(clf,       'logistic_model.pkl')"
      ],
      "metadata": {
        "id": "QGETOMqjMQ0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "\n",
        "classes = ['Negative', 'Positive']\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Матрица ошибок')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes, rotation=45)\n",
        "plt.yticks(tick_marks, classes)\n",
        "thresh = cm.max() / 2.\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.ylabel('Настоящий класс')\n",
        "plt.xlabel('Предсказанный класс')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aPp6WDL8rql9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = vec.get_feature_names_out()\n",
        "coefs = clf.coef_[0]\n",
        "weights_df = pd.DataFrame({\n",
        "    'word': feature_names,\n",
        "    'weight': coefs\n",
        "})\n",
        "\n",
        "top_positive = weights_df.sort_values('weight', ascending=False).head(20)\n",
        "top_negative = weights_df.sort_values('weight', ascending=True).head(20)\n",
        "\n",
        "print(\"Топ-20 слов, которые сильно указывают на positive:\")\n",
        "print(top_positive)\n",
        "\n",
        "print(\"\\nТоп-20 слов, которые сильно указывают на negative:\")\n",
        "print(top_negative)"
      ],
      "metadata": {
        "id": "ESc_ETp1q6De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# tf-idf без лемматизации\n",
        "\n",
        "df['final_text_no_lemm'] = [\" \".join(sentence) for sentence in tokenize_text_wsws]\n",
        "# print(\"Размер df:\", df.shape)\n",
        "# print(\"Количество текстов:\", len(df['final_text']))\n",
        "X = df['final_text_no_lemm']\n",
        "y = df['sentiment']\n",
        "y = y.map({'positive': 1, 'negative': 0})\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "vec = TfidfVectorizer(ngram_range=n2)\n",
        "vec_train = vec.fit_transform(X_train)\n",
        "vec_test = vec.transform(X_test)\n",
        "\n",
        "# нормализация\n",
        "scaler = MaxAbsScaler()\n",
        "vec_train = scaler.fit_transform(vec_train)\n",
        "vec_test = scaler.transform(vec_test)\n",
        "\n",
        "# обучение с tf-idf и лемматизацией\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "clf = LogisticRegression(max_iter=200, random_state=42)\n",
        "clf.fit(vec_train, y_train)\n",
        "pred = clf.predict(vec_test)\n",
        "print(classification_report(y_test, pred))\n"
      ],
      "metadata": {
        "id": "DW8uro4pMynm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# разбиение датасета с валидацией и обучение\n",
        "\n",
        "df['final_text'] = [\" \".join(sentence) for sentence in lemmatized_text]\n",
        "# print(\"Размер df:\", df.shape)\n",
        "# print(\"Количество текстов:\", len(df['final_text']))\n",
        "X = df['final_text']\n",
        "y = df['sentiment']\n",
        "y = y.map({'positive': 1, 'negative': 0})\n",
        "# на тест 30%\n",
        "X_rest, X_test, y_rest, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# на валидацию 20%\n",
        "val_size_relative = 0.20\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_rest, y_rest,\n",
        "    test_size=val_size_relative,\n",
        "    random_state=42,\n",
        "    stratify=y_rest\n",
        ")\n",
        "\n",
        "clf = LogisticRegression(max_iter=200, random_state=42, C=1.0)\n",
        "clf.fit(vec.fit_transform(X_train), y_train)\n",
        "\n",
        "\n",
        "pred_val = clf.predict(vec.transform(X_val))\n",
        "print(\"Validation:\")\n",
        "print(classification_report(y_val, pred_val))\n",
        "\n",
        "\n",
        "# pred_test = clf.predict(vec.transform(X_test))\n",
        "# print(\"\\nFinal Test:\")\n",
        "# print(classification_report(y_test, pred_test))"
      ],
      "metadata": {
        "id": "kyAsYyYmt-7D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}